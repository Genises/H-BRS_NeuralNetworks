{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Living skin detection # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: Jan Zahn, Jonas Meier, Thomas Wiktorin ##\n",
    "\n",
    "## Task ##\n",
    "\n",
    "Train a classifier which is able to distinguish between living and dead materials with highest success rate\n",
    "The used NN should have few most distinctive input features i.e. spectral lines\n",
    "\n",
    "Steps:\n",
    "\n",
    "Read all XLS and CSV data files into Python.  \n",
    "Visualize them and based on your insight suggest / try some decent feature classifiers.  \n",
    "Select them, train them and validate them.  \n",
    "Analyse runtime and memory footprint.  \n",
    "Argue why your solution is appropriate.  \n",
    "\n",
    "EXTENSION :\n",
    "please implement and compare the \"Living Skin\" detection using  MLP and  SVMs and radial basis functions RBFs.  \n",
    "Imporant metrics are especailly confusion-matrix, precsion and recall, many more may be runtime, memory foot step  \n",
    "Train time etc  \n",
    "\n",
    "\n",
    "## Explanation:##\n",
    "\n",
    "BUG NOTE: We shuffle the data to get a normalised distribution of datatypes. Sometimes the random shuffle does not shuffle the the data-matrix and class-matrix the same way order, resulting in an unusable model with 50% accuraxy. If this happens, re-run the program.\n",
    "\n",
    "In our first program we chose to use all 121 values on the input layer, and see how accurate our model is. We reached over 96 percent accuracy consistently. Using different sizes of epochs/batch_size/validation data/training data/test data/hidden layers did not change our results in huge ways. However, I believe the size of the hidden layers is better to be small due to overfitting.   \n",
    "\n",
    "One problem existed in the unequal representation of classes. The \"living Material\" has just 6 examples, while the other class is represented by 171 data sets. To reach a high accuracy, our model just classified everything as \"dead material\" and had instant high training and validation accuracy. Of course, this accuracy is meaningless, since we are often interested in the under represented class.  \n",
    "\n",
    "At first, we tried to weight our examples by the means of sensitivity and specificity.  \n",
    "\n",
    "sensitivity = true positives / positives  \n",
    "specificity = true negatives / negatives  \n",
    "\n",
    "Our model always predicts dead material and therefore has a sensitivity of 0 and specificity of 1.  \n",
    "We want to achieve a model that gets close to both sensitivity and specificity beeing 1.  \n",
    "\n",
    "Weighting did not fix our issues, so we increased the number of positive examples by duplicating them, until the numbers for each class were equal.  \n",
    "This fixed our issue and we got high accuracy (>95%) and a sensitivity & specificity value close to 1.\n",
    "\n",
    "In the second program we reduced our number of features, otherwise it is the same. \n",
    "We reduced the input layer from 121 to 12.\n",
    "\n",
    "We looked at the second derivative and the concavity (see program 2 graphs) seemed to provide a few distinct values, important for our few features. We chose the points where the second derivative is in its extremes for both min and max, resulting in a total of 12 points.   \n",
    "This resulted in a higher accuracy (>98%) and a more stable learning (see last graphs in program two)  \n",
    "\n",
    "Runtime for an prediction of our model with 12 features takes:  \n",
    "0.0001302809675962635 seconds  \n",
    "And with 121:  \n",
    "0.0001329888860936989 seconds  \n",
    "  \n",
    "340 evaluations with 121 features take:  \n",
    "0.00032795901779536507 seconds  \n",
    "  \n",
    "340 evaluations with 12 features take:  \n",
    "0.000109520259115925  \n",
    "\n",
    "We get a slight improvement, but with our data and network size, runtime and memory does not seem to matter much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#-------------------------------------------------------------------------------Get Data\n",
    "dataFilesOldNegatives = [\"Fleisch\", \"Holz\", \"Leder\", \"Stoff\"]\n",
    "dataFilesOldPositives = [\"Referenz-Haut_6-Klassen\"]\n",
    "\n",
    "dataFilesNewNegatives = [\"2016material\", \"2016material-fake\"]\n",
    "dataFilesNewPositives = [\"2016skin\"]\n",
    "\n",
    "def importData(fileNames):\n",
    "    data = np.array([]);\n",
    "    xPoints = np.array([]);\n",
    "    for dataType in fileNames:\n",
    "        with open(\"Archiv\\\\\" + dataType + '.csv', mode='r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "                \n",
    "                newList = list(csv_reader)      \n",
    "                for index, row in enumerate(newList):\n",
    "                   newList[index] = [value.replace(',', '.') for value in row]\n",
    "                \n",
    "                newData = np.asarray(newList)\n",
    "                newData = np.delete(newData, (0), axis=0)\n",
    "                if xPoints.size == 0:\n",
    "                    xPoints = newData[:,0]\n",
    "                newData = np.delete(newData,(0), axis=1)\n",
    "                \n",
    "                if data.size == 0:\n",
    "                    data = newData.transpose()\n",
    "                else:\n",
    "                    data = np.append(data, newData.transpose(),axis=0)\n",
    "    \n",
    "        data = data.astype(np.float)\n",
    "        xPoints =  xPoints.astype(np.float)    \n",
    "    return xPoints, data \n",
    "\n",
    "\n",
    "#Negatives Old\n",
    "xValuesOld, dataNegativesOld = importData(dataFilesOldNegatives)\n",
    "\n",
    "#Positives Old\n",
    "_, dataPositivesOld = importData(dataFilesOldPositives)\n",
    "\n",
    "#Negatives New\n",
    "xValuesNew, dataNegativesNew = importData(dataFilesNewNegatives)\n",
    "\n",
    "#Positives New\n",
    "_, dataPositivesNew = importData(dataFilesNewPositives)\n",
    "\n",
    "#Increase positives to deal with unbalance class (maybe not needed with 2016 set)\n",
    "dataPositives = np.tile(dataPositivesOld,(28,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xValuesOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9dfb6d42db5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplottSpectrals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataNegativesOld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Negative old\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mplottSpectrals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Positive old\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplottSpectrals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataNegativesNew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Negative new\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xValuesOld' is not defined"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------Plott Data\n",
    "def plottSpectrals(x,y,z, format=\"\"):\n",
    "    for row in y:\n",
    "        plt.plot(x,row, format)\n",
    "    plt.title(z)\n",
    "    plt.show()\n",
    "        \n",
    "plottSpectrals(xValuesOld,dataNegativesOld, \"Negative old\")\n",
    "plottSpectrals(xValuesOld,dataPositivesOld, \"Positive old\")\n",
    "plottSpectrals(xValuesNew,dataNegativesNew, \"Negative new\")\n",
    "plottSpectrals(xValuesNew,dataPositivesNew, \"Positive new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xValuesOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0b8a37653ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#--------------------------------------------------------------------------------Some Information on Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Old xValue data: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Old negative data: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataNegativesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Old positive data: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"New xValue data: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xValuesOld' is not defined"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------Some Information on Data\n",
    "print(\"Old xValue data: \" + str(xValuesOld.shape))\n",
    "print(\"Old negative data: \" + str(dataNegativesOld.shape))\n",
    "print('Old positive data: ' + str(dataPositivesOld.shape))\n",
    "print(\"New xValue data: \" + str(xValuesNew.shape))\n",
    "print('New negative data: ' + str(dataNegativesNew.shape))\n",
    "print('New positive data: ' + str(dataPositivesNew.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataNegativesNew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f30e28e4d8e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Delete NaN at the end of new files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdataNegativesNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataNegativesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdataPositivesNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataPositivesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mxValuesNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxValuesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNegativesNew' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------Combine different Data\n",
    "#Old wavelength 400-1600 ;                           in steps of 10\n",
    "#New wavelength 670-1690 (everything after is NaN);  in steps of 1 \n",
    "\n",
    "#Delete NaN at the end of new files\n",
    "dataNegativesNew = dataNegativesNew[:,:xValuesNew.size-10]\n",
    "dataPositivesNew = dataPositivesNew[:,:xValuesNew.size-10]\n",
    "xValuesNew = xValuesNew[:xValuesNew.size-10]\n",
    "\n",
    "#InterpolateOldData to match new \n",
    "tmp_positive_old = np.empty((len(dataPositivesOld),1200))\n",
    "tmp_negative_old = np.empty((len(dataNegativesOld),1200))\n",
    "xValuesAlteredOld = np.asarray(range(400,1600))\n",
    "\n",
    "for i in range(len(dataPositivesOld)):\n",
    "    tmp_positive_old[i,:] = np.interp(xValuesAlteredOld,xValuesOld,dataPositivesOld[i,:])\n",
    "\n",
    "for i in range(len(dataNegativesOld)):\n",
    "    tmp_negative_old[i,:] = np.interp(xValuesAlteredOld,xValuesOld,dataNegativesOld[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xValuesAlteredOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-927df8016ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#---------------------------------------------------------------------------------Print new data format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mxValuesOld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxValuesAlteredOld\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataNegativesOld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_negative_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataPositivesOld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_positive_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xValuesAlteredOld' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------Print new data format\n",
    "\n",
    "xValuesOld = xValuesAlteredOld\n",
    "dataNegativesOld = tmp_negative_old\n",
    "dataPositivesOld = tmp_positive_old\n",
    "\n",
    "print(\"After interpolation:\")\n",
    "print(\"Old negative data: \" + str(dataNegativesOld.shape))\n",
    "print('Old positive data: ' + str(dataPositivesOld.shape))\n",
    "print('New negative data: ' + str(dataNegativesNew.shape))\n",
    "print('New positive data: ' + str(dataPositivesNew.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataNegativesOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f88817089b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#---------------------------------------------------------------------------------Cut off old data to match new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataNegativesOld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataNegativesOld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataPositivesOld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataNegativesNew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataNegativesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxValuesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mxValuesNew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNegativesOld' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------Cut off old data to match new data\n",
    "\n",
    "dataNegativesOld = dataNegativesOld[:,int(xValuesNew[0]-xValuesOld[0]):]\n",
    "dataPositivesOld = dataPositivesOld[:,int(xValuesNew[0]-xValuesOld[0]):]\n",
    "dataNegativesNew = dataNegativesNew[:,:int(xValuesOld[xValuesOld.size-1]-xValuesNew[0])+1]\n",
    "dataPositivesNew = dataPositivesNew[:,:int(xValuesOld[xValuesOld.size-1]-xValuesNew[0])+1]\n",
    "\n",
    "xValuesOld = np.asarray(range(670,1600))\n",
    "xValuesNew = xValuesOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cutting:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataNegativesOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e5b54da3e523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After cutting:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Old negative data: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataNegativesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Old positive data: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New negative data: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataNegativesNew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNegativesOld' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------Print new data format\n",
    "\n",
    "print(\"After cutting:\")\n",
    "print(\"Old negative data: \" + str(dataNegativesOld.shape))\n",
    "print('Old positive data: ' + str(dataPositivesOld.shape))\n",
    "print('New negative data: ' + str(dataNegativesNew.shape))\n",
    "print('New positive data: ' + str(dataPositivesNew.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataNegativesOld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9edd5342e9bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Stores average value of each measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdataSet\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataNegativesOld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataNegativesNew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataPositivesNew\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# Average for each measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNegativesOld' is not defined"
     ]
    }
   ],
   "source": [
    "# Normalisation of ys over all data\n",
    "\n",
    "# Stores average value of each measure\n",
    "avg = []\n",
    "for dataSet in [dataNegativesOld, dataPositivesOld, dataNegativesNew, dataPositivesNew]:\n",
    "    for measure in dataSet:\n",
    "        # Average for each measure\n",
    "        avg.append(np.average(measure))\n",
    "avg = np.average(avg)\n",
    "print(\"Average value of all measurements is\", avg)\n",
    "\n",
    "# Use average to normalise data\n",
    "for dataSet in [dataNegativesOld, dataPositivesOld, dataNegativesNew, dataPositivesNew]:\n",
    "    for index, measure in enumerate(dataSet):\n",
    "        # Average for each measure\n",
    "        dataSet[index] = measure - avg\n",
    "\n",
    "#--------------------------------------------------------------------------------Plott Data        \n",
    "plottSpectrals(xValuesOld,dataNegativesOld, \"Negative old normalised\")\n",
    "plottSpectrals(xValuesOld,dataPositivesOld, \"Positive old normalised\")\n",
    "plottSpectrals(xValuesNew,dataNegativesNew, \"Negative new normalised\")\n",
    "plottSpectrals(xValuesNew,dataPositivesNew, \"Positive new normalised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4a74cc377f30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Extra Value for less Features (maybe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# GradientData = 2nd derivation of input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgradientData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataPositivesOld\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------New Features\n",
    "#Extra Value for less Features (maybe)\n",
    "# GradientData = 2nd derivation of input data\n",
    "gradientData = np.empty([dataPositivesOld.shape[0],dataPositivesOld.shape[1]]);\n",
    "i = 0;\n",
    "for row in dataPositivesOld:\n",
    "    gradient = np.gradient(np.gradient(row)) #where does the gradient change fastest\n",
    "    plt.plot(xValuesOld,gradient)\n",
    "    gradientData[i] = gradient\n",
    "    i += 1\n",
    "    \n",
    "plt.title(\"Change of gradient for old positive data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-85fe1a64266f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnumber_of_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmaxGradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mminGradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgradientData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------Split Up Data\n",
    "#hardcoded\n",
    "# Get gradients\n",
    "#maxGradients = gradientData.argmax(axis=1)\n",
    "#minGradients = gradientData.argmin(axis=1)\n",
    "#smallXValues = np.append(maxGradients,minGradients,axis=0)\n",
    "\n",
    "number_of_values = 5\n",
    "\n",
    "maxGradients = np.array([], dtype=np.int)\n",
    "minGradients = np.array([], dtype=np.int)\n",
    "for entry in gradientData:\n",
    "    maxGradients = np.append(maxGradients, entry.argsort()[-number_of_values:][::-1])\n",
    "    minGradients = np.append(minGradients, entry.argsort()[:number_of_values])\n",
    "\n",
    "smallXValues = np.append(maxGradients,minGradients,axis=0)\n",
    "    \n",
    "# Remove duplicate wavelenghts\n",
    "smallXValues = np.unique(smallXValues)\n",
    "print(\"Wavelengths to use for further operations:\", smallXValues, \"nm.\")\n",
    "\n",
    "# Small features = only wavelengths where gradient is max or min\n",
    "smallFeaturesPositiveReal = dataPositivesOld[:,smallXValues]\n",
    "smallFeaturesPositive = np.tile(smallFeaturesPositiveReal,(28,1)) #Increase positives to deal with unbalance class\n",
    "smallFeature = dataNegativesOld[:,smallXValues]\n",
    "\n",
    "#[1,0] is dead, [0.1] alive\n",
    "#CompleteSet Small Version\n",
    "compSet = np.append(smallFeature, smallFeaturesPositive ,axis=0)\n",
    "classifcSet = np.append(np.tile([1,0],(smallFeature.shape[0],1)),np.tile([0,1],(smallFeaturesPositive.shape[0],1)),axis=0)\n",
    "\n",
    "#shuffle data together\n",
    "mix = np.random.permutation(len(compSet))\n",
    "compSet = compSet[mix]\n",
    "classifcSet = classifcSet[mix]\n",
    "\n",
    "#Split in training and test Data\n",
    "#trainingSet = compSet[:200]\n",
    "#trainingLabelSet =  classifcSet[:200]\n",
    "\n",
    "trainingSet = compSet[:300]\n",
    "trainingLabelSet =  classifcSet[:300]\n",
    "\n",
    "#validationSet = compSet[200:300]\n",
    "#validationLabelSet = classifcSet[200:300]\n",
    "\n",
    "testSet = compSet[300:]\n",
    "testLabelSet = classifcSet[300:]\n",
    "\n",
    "print(smallXValues.shape)\n",
    "print(trainingSet.shape)\n",
    "print(smallFeature.shape)\n",
    "print(smallFeaturesPositive.shape)\n",
    "\n",
    "plottSpectrals(smallXValues,smallFeature, \"Negative features\", \"o\")\n",
    "plottSpectrals(smallXValues,smallFeaturesPositive, \"Positive features\", \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Das angegebene Modul wurde nicht gefunden.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-75f8f775373b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnumber_of_hidden_neurons\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhidden_neuron_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Systemverwaltung\\Anaconda3\\envs\\Tensorflow 2\\lib\\imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------Build Model with tensorflow\n",
    "\n",
    "hidden_neuron_size = [5,10,15,20,50,100,500]\n",
    "loss_array = []\n",
    "acc_array = []\n",
    "\n",
    "for number_of_hidden_neurons in hidden_neuron_size:\n",
    "    from tensorflow.keras import backend as K\n",
    "\n",
    "    # https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        one_weight = 1\n",
    "        zero_weight = 1\n",
    "\n",
    "        # Original binary crossentropy (see losses.py):\n",
    "        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        print(y_true)\n",
    "\n",
    "        # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "\n",
    "\n",
    "    model = keras.Sequential() #Single input-output\n",
    "    model.add(keras.layers.Dense(number_of_hidden_neurons, activation=tf.nn.relu, input_shape=(trainingSet.shape[1],))) #fully-conndected = dense, with 16 units, relu: rectified linear unit\n",
    "    model.add(keras.layers.Dense(2, activation=tf.nn.softmax)) #Cofidence level\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    #Optimizer and loss function\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(), #or sgd(stochastic gradient descent optimizer: keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "                  loss='binary_crossentropy', #or mean_squared_error (our target is not in the continuos space), but binary seems to deal better with probabilitis\n",
    "                  #loss=weighted_binary_crossentropy,\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "\n",
    "\n",
    "    #Weighted class did not solve the unequal class problem\n",
    "    #[1,0] is dead, [0.1] alive\n",
    "\n",
    "    # Create sample weights\n",
    "    # Make positive samples count more\n",
    "    weights = []\n",
    "    for index, entry in enumerate(trainingSet):\n",
    "        if trainingLabelSet[index][0] == 0:\n",
    "            weights.append(1)\n",
    "        else:\n",
    "            weights.append(2)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    #class_weight=[3, .5]\n",
    "\n",
    "    #Train model for 50 epochs in batches of 3 samples\n",
    "    history = model.fit(trainingSet,\n",
    "                        trainingLabelSet,\n",
    "                        epochs=50,\n",
    "                        batch_size=10,  #the bigger the more memory space needed\n",
    "                        validation_split=0.2,\n",
    "                        #verbose=1,\n",
    "                        verbose=0,\n",
    "                        sample_weight=weights,\n",
    "                        #class_weight=class_weight\n",
    "                        )\n",
    "\n",
    "    print()\n",
    "    results = model.evaluate(testSet, testLabelSet)\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for entry in testLabelSet:\n",
    "        if entry[0] == 0:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "\n",
    "    print(\"Positives in testset = \", positive)\n",
    "    print(\"Negatives in testset = \", negative)\n",
    "\n",
    "    for index, metric in enumerate(model.metrics_names):\n",
    "        print(metric, \": \", results[index])\n",
    "        # Loss\n",
    "        if index == 0:\n",
    "            loss_array.append(results[index])\n",
    "        else:\n",
    "            acc_array.append(results[index])\n",
    "            \n",
    "\n",
    "    # Creating the Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(testSet)\n",
    "    y_test = testLabelSet\n",
    "    #print(y_test)\n",
    "    #print(y_pred)\n",
    "    #print(y_pred.round())\n",
    "    cm = confusion_matrix(y_test.argmax(axis=1), y_pred.round().argmax(axis=1))\n",
    "    print(cm)\n",
    "\n",
    "plt.plot(hidden_neuron_size, loss_array, label=\"Loss\")\n",
    "plt.plot(hidden_neuron_size, acc_array, label=\"Accuracy\")\n",
    "plt.xlabel(\"Number of hidden neurons\")\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-aabef5831f6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#--------------------------------------------------------------------------------Print Results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Plot accuracy and loss over time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#--------------------------------------------------------------------------------Print Results\n",
    "#Plot accuracy and loss over time\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------Some Time estimation\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "result = model.predict(smallFeature)\n",
    "end = timer()\n",
    "ms = (end - start) / 1000\n",
    "print(\"%fms\"% ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.rueckstiess.net/research/snippets/show/72d2363e\n",
    "from scipy import *\n",
    "from scipy.linalg import norm, pinv\n",
    " \n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "class RBF:\n",
    "     \n",
    "    def __init__(self, indim, numCenters, outdim):\n",
    "        self.indim = indim\n",
    "        self.outdim = outdim\n",
    "        self.numCenters = numCenters\n",
    "        self.centers = [random.uniform(-1, 1, indim) for i in range(numCenters)]\n",
    "        self.beta = 8\n",
    "        self.W = random.random((self.numCenters, self.outdim))\n",
    "         \n",
    "    def _basisfunc(self, c, d):\n",
    "        assert len(d) == self.indim\n",
    "        return exp(-self.beta * norm(c-d)**2)\n",
    "     \n",
    "    def _calcAct(self, X):\n",
    "        # calculate activations of RBFs\n",
    "        G = zeros((X.shape[0], self.numCenters), float)\n",
    "        for ci, c in enumerate(self.centers):\n",
    "            for xi, x in enumerate(X):\n",
    "                G[xi,ci] = self._basisfunc(c, x)\n",
    "        return G\n",
    "     \n",
    "    def train(self, X, Y):\n",
    "        \"\"\" X: matrix of dimensions n x indim \n",
    "            y: column vector of dimension n x 1 \"\"\"\n",
    "         \n",
    "        # choose random center vectors from training set\n",
    "        rnd_idx = random.permutation(X.shape[0])[:self.numCenters]\n",
    "        self.centers = [X[i,:] for i in rnd_idx]\n",
    "         \n",
    "        print (\"center\", self.centers)\n",
    "        # calculate activations of RBFs\n",
    "        G = self._calcAct(X)\n",
    "        print (G)\n",
    "         \n",
    "        # calculate output weights (pseudoinverse)\n",
    "        self.W = dot(pinv(G), Y)\n",
    "         \n",
    "    def test(self, X):\n",
    "        \"\"\" X: matrix of dimensions n x indim \"\"\"\n",
    "         \n",
    "        G = self._calcAct(X)\n",
    "        Y = dot(G, self.W)\n",
    "        return Y\n",
    " \n",
    "    \n",
    "myRFB = RBF(12,3,2)\n",
    "myRFB.train(trainingSet, trainingLabelSet)\n",
    "myRFB.test(testSet)\n",
    "\n",
    "# plot rbfs\n",
    "plt.plot(myRFB.centers, zeros(myRFB.numCenters), 'gs')\n",
    "\n",
    "\n",
    "for c in myRFB.centers:\n",
    "    # RF prediction lines\n",
    "    cx = c\n",
    "    #cx = np.arange(c-0.7, c+0.7, 0.01)\n",
    "    cy = [myRFB._basisfunc(array([cx_]), array([c])) for cx_ in cx]\n",
    "    plt.plot(cx, cy, '-', color='gray', linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = tf.convert_to_tensor([0,1], dtype=np.int32)\n",
    "predict = tf.convert_to_tensor([0,1], dtype=np.int32)\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return keras.mean(keras.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "print(binary_crossentropy(true, predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
